# Shield AI - Detector de Discurso de Ã“dio e Bullying em PortuguÃªs

Este projeto utiliza tÃ©cnicas de Processamento de Linguagem Natural (PLN) e aprendizado de mÃ¡quina para detectar automaticamente discurso de Ã³dio, bullying e toxicidade em comentÃ¡rios.

## ğŸš€ Tecnologias Utilizadas

- Python 3.x
- TensorFlow / Keras
- NLTK
- Pandas, NumPy, Matplotlib
- Gradio (interface web interativa)

## ğŸ“‹ DescriÃ§Ã£o

O sistema foi desenvolvido para identificar automaticamente diferentes tipos de toxicidade em textos, incluindo:
- TÃ³xico
- Extremamente TÃ³xico
- Obsceno
- AmeaÃ§a
- Insulto
- Ã“dio IdentitÃ¡rio

O modelo utiliza uma rede neural LSTM bidirecional treinada sobre um dataset pÃºblico de comentÃ¡rios rotulados.

## ğŸ› ï¸ Como Executar

1. **Clone o repositÃ³rio:**
   ```bash
   git clone https://github.com/Flavio-sv/Shield-AI.git
   ```

3. **Execute o notebook Jupyter:**
   ```bash
   Shield_AI.ipynb
   ```

> âš ï¸ **AtenÃ§Ã£o:** Use o Python 3.12.0

## ğŸ“Š Resultados

O modelo foi executado durante 10 Ã©pocas de treinamento.

- **PrecisÃ£o:** 0.86
- **Recall:** 0.87
- **AcurÃ¡cia:** 0.33
- **Loss final:** 0.0780
- **Val_loss final:** 0.0705

O modelo apresenta bom desempenho na detecÃ§Ã£o de toxicidade, especialmente em classes mais frequentes.

## âš ï¸ LimitaÃ§Ãµes

- Pode apresentar falsos positivos em textos ambÃ­guos ou sarcÃ¡sticos.
- Depende da qualidade e representatividade do dataset.
- Dificuldade em interpretar contexto amplo ou gÃ­rias regionais.

## ğŸ“„ LicenÃ§a

Este projeto Ã© de uso acadÃªmico.
